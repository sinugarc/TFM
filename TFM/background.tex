\cleardoublepage

\chapter{Background}
\label{Ch2:Back}

\section{Metamorphic testing in classical computing}
\label{Ch2.1:Metamorphic}

\section{Quantum computing}
\label{Ch2.2:Quantum}
Elements to be included:
\begin{itemize}
    \item Relaxation time
\end{itemize}

\newpage

\section{Testing in Quantum}
\label{Ch2.3:TQuantum}

After providing a brief introduction to quantum computing, let us allocate some time to delve into the latest developments in the field of quantum testing. This section will be divided into two different perspectives. Firstly, we will explore the testing of Quantum Platforms, with a specific focus on Qiskit and how it can evolve for different platforms. Subsequently, we will continue with various approaches and techniques for testing quantum programs, hereafter referred to as QP.

\subsection{Quantum computing platforms}
\label{Ch2.3.1:TPlat}

As mentioned earlier, we will begin by examining the testing of quantum platforms. We will introduce these approaches in chronological order and the authors will even assess how they compare to each other, observing improvements over time.

\subsubsection{QDiff}
\label{Ch2.3.1:QDiff}

QDiff, differential testing of quantum software stacks (QSS), is the approached presented by Jiyuan Wang et all. in their 2021 article titled \textit{"QDiff: differential testing of quantum software stacks"}\cite{wang2021qdiff}, they introduce it as a novel differential testing technique for QSS.\newline

Their motivation behind this work arises from the increasing interest in quantum computing and the development of QSS as a high-level languages for quantum programming. These languages aim to shield users from the mathematical and physical complexities inherent in quantum computing. Another motivation is to offer a potential solution for identifying failures within QSS, which aids in resolving the confusion and incoherence that often arises when a user reports bugs. When reviewing the collection of issues reported for each QSS, these problems frequently reveal inconsistencies in identifying the potential source of the bug or determining whether the issue originates from the probabilistic or noisy nature of quantum indeterminacy. \newline

The authors idea is to be able to perform testing in the three most widely used QSS \cite{larose2019overview} at that time: Qiskit, Cirq and Pyquil. Now, let us explore the workflow (Figure \ref{Fig:QDiffWorkOverflow}) and key innovations in  QDiff, which will include their propose solutions for technical challenges that complicate testing within QSS.

\newpage
\begin{itemize}
    \item QP generator: In order to fulfil our needs while testing QSS, we need to generate semantically equivalent programs. The authors proposed a set of equivalent gate transformation rules designed to preserve program semantics. These seven rules have been derived from a detail analysis of each gate and their behaviour when deployed sequentially.
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.6\textwidth]{TFM/photos/QDiffRules.png}
        \caption{Semantic preserving rules \cite{wang2021qdiff}} 
        \label{Fig:QDiffRules}
    \end{figure}

    Followed by mutation modifications, QDiff utilises this process to diversify the pool of input programs. The initial seed consists of 6 QPs, and through mutation, it will be expanded to 730 variants. Quantum mutant operators used in QDiff:
        \begin{itemize}
            \item[] M1: Gate insertion/deletion.
            \item[] M2: Gate change.
            \item[] M3: Gate swap.
            \item[] M4: Qubit change.
        \end{itemize}

    \item Filtering QP based on whether they are worth running. This determination relies on the execution system and the specific QP. A circuit is deemed suitable for execution on quantum hardware or a noisy simulator if it adheres to two key thresholds:
    
    \begin{itemize}
        \item[-] The maximum number of gates, determined by average execution gate and T1 time for the system, where T1 represents the decoherence time for a qubit. 
        \item[-] The maximum number of 2-qubit gates, determined by the error rate tolerance willing to be added by the user  the final measurements 
    \end{itemize}

    A concrete example of these boundaries is provided in \textit{IV. QDIFF APPROACH - B. Quantum Simulation and Hardware Execution} \cite{wang2021qdiff}.

    \item Comparing outcomes derived from two equivalent quantum programs. The authors employ two similar techniques discussed in \textit{IV. QDIFF APPROACH - C. Equivalence Checking via Distribution Comparison} \cite{wang2021qdiff} and prior works \cite{chan2014optimal}\cite{aaronson2016complexity}\cite{cross2019validating}. Let use outline both methods.\newline
    
    Firstly, it is imperative to establish a threshold $t$ representing the maximum acceptable distance and a significance level $p$. Afterwards, the minimum number of executions required for statistically guaranteed comparison is calculated. Following the executions, empirical distribution functions (EDFs) are computed.

    \begin{itemize}
        \item[-] Kolmogorov-Smirnov (K-S) distance: This metric characterise the K-S distance as the most substantial difference between the two EDFs. If this difference is less than $t$, the QPs are deemed similar.
        \item[-] Cross entropy: The comparison involves evaluating the total entropy of both EDFs, equally constrained by the threshold $t$.
    \end{itemize}

    \item Reporting divergences. Once a divergence has been found, the system will identify the difference between both QPs. Then it difference between: backend, frontend and API gate implementation.
    
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.6\textwidth]{TFM/photos/QDiffWorkoverflow.png}
        \caption{QDiff overview \cite{wang2021qdiff}} 
        \label{Fig:QDiffWorkOverflow}
    \end{figure}

    We should now conclude by highlighting the key contributions and outcomes of these experiments:

\begin{itemize}
    \item QDiff successfully evaluated more than one QSS.
    \item A total of 14799 QPs were generated, organised into 730 sets of semantically equivalent circuits.
    \item Among the 730 sets, 33 differing outcomes were identified, all which will undergo manual checking.
    \begin{itemize}
        \item[-] Out of these, 4 discrepancies resulted in simulator crashes, while the remaining divergences exceeded expected noise levels on IBM hardware.
        \item[-] Six distinct sources of instability were identified, comprising 4 software crash bugs in Pyquil and Cirq, along with 2 root causes potentially explaining 25 out of 29 cases of divergence beyond anticipated noise on IBM hardware, attributed to unreliable connections between two qubits.
    \end{itemize}
    
\end{itemize}

\subsubsection{MorhpQ}
\label{Ch2.3.1:MorphQ}
Matteo Paltenghi and Michael Pradel presented in May 2023 their latest work on testing QQS, introducing metamorphic testing as a novel approach for quantum platforms. Their innovative method involves the design of metamorphic rules for Qiskit.3 Their article, titled: \textit{"MorphQ: Metamorphic testing of the qiskit quantum computing platform"} \cite{paltenghi2023morphq}, further develops this concept, presenting a fresh perspective on generating quantum programs using a defined grammar. This advancement brings us closer to the idea of automating the testing process, eliminating the need for a library of quantum programs as generation seed, which was the traditional approach up to this point.\newline

MorphQ will focus on Qiskit, as quantum platform, employing metamorphic testing to address specific challenges presented in quantum computing. The oracle problem will be avoided as it will be resolved by the use of MR, eliminating the need for a detailed specification of expected input behaviour. Let us provide a general overview of MorphQ as presented by the authors, then we will focus on the key innovations within each of its distinct components .

\begin{figure}[H]
        \centering
        \includegraphics[width=0.58\textwidth]{TFM/photos/MorphQOverview.png}
        \caption{MorphQ overview \cite{paltenghi2023morphq}} 
        \label{Fig:MorphQOverview}
\end{figure}

\vspace{-12pt}
As previously mentioned, one of the authors' significant contributions lies in their approach to program generation. Since the focus is on testing the quantum platform, the primary objective of this proposal is to create syntactically correct quantum programs, thus preventing execution crashes. These this automating generated programs will serve as test suit for the QSS. To begin, Paltenghi and Pradel introduced the grammar that will guide the program generation process. This grammar follows the typical recursive structure found in computing. A subset of it is illustrated in Figure \ref{Fig:MorphQGrammar}, as provided by the authors in their paper. You can access all the related information about the grammar and the rest of the article on their \hyperlink{https://github.com/sola-st/MorphQ-Quantum-Qiskit-Testing-ICSE-23}{GitHub} page. (Visible link, hided link, footnote, both?)

\vspace{-8pt}
\begin{figure}[H]
        \centering
        \includegraphics[width=0.56\textwidth]{TFM/photos/MorphQGrammar.png}
        \caption{Subset of the QP generation grammar \cite{paltenghi2023morphq}} 
        \label{Fig:MorphQGrammar}
\end{figure}

The purpose of defining this grammar is to ensure that invalid quantum programs are avoided. A random approach would not be ideal, as it would likely generate a high number of invalid programs. Additionally, the authors will impose a constraint on the number of gates per program, limiting it to 30. Higher amount of gates can increase considerably the execution time due to the complexity of quantum operations. Their commitment towards keeping execution times within reasonable limits, lies in the decision of the authors to only use simulators, where the size of the matrix defining a single operator grows exponentially with the number of qubits involved.\newline

Once we have been able to generate the source input for the testing desired, the authors defined the metamorphic rules that Qiskit should fulfil classifying them in 3 categories: Circuit transformation which modify the circuit, representation transformations which change the intermediate representation of QP and execution transformations, which affect the execution environment, we could observe this metamorphic rules in Figure \ref{Fig:MorphQMR}. 

\begin{figure}[H]
        \centering
        \includegraphics[width=0.6\textwidth]{TFM/photos/MorphQMR.png}
        \caption{Qiskit MR used by QMorph \cite{paltenghi2023morphq}} 
        \label{Fig:MorphQMR}
\end{figure}

The expected relationship between source and follow-up outputs is "equivalence", with the only exception of non-semantic-preserving transformations, specifically, the change of qubit order and partitioned execution which will need a posterior treatment before comparing behaviours. This is why we will continue applying metamorphic transformations unless one of the exceptions mentioned above is introduced. This transformations are executed through python AST or/and sectioning source code with python matching technique, adding needed elements and reconstructing the code.\newline

In the final step of MorphQ, which involves execution and behaviour checks, the primary assessment is crash check. Identifying a crash in the follow-up program marks a critical failure. As we are all aware, when executing quantum programs without accounting for quantum noises, the result can be non-deterministic linked to the final state amplitudes, reflecting the well-known probabilistic nature of quantum measurement. Consequently, we would need to execute the programs a specific number of shots to compare two different distributions and decide if the output is "equivalent" or we have possibly found a failure. To determine this 
required number of executions, the authors follow the same technique as the one used in QDiff, employing the \textbf{L1 norm }\cite{chan2014optimal}. The distribution are compared using Kolmogorov-Smirnov test (p-value < 5\%)\newline
 
Another noteworthy aspect is the management of warning messages related to crashes. The authors implemented a semi-automatic clustering approach for these warnings, aiming to abstract from program-specific references. Afterwards, a random selection process is applied to each cluster, and the chosen programs undergo manual inspection. During these inspections, transformations are reversed step by step until the one responsible for the fault is reached. Then, they use \textbf{delta debugging} until they identify the minimal sequence of operations to trigger the crash. \newline

Let us summarise the key results and contributions of MorphQ, keeping in mind that the experiments were constrained by a 48-hour execution time-frame. The authors also adapted QDiff to the same execution time-frame to be able to compare results.

\begin{itemize}
    \item QP generator:
    \begin{itemize}
        \item Automatically generated 8360 quantum programs .
        \item Source-QP executed without crashing.
        \item Wide range of programs produced (Figure \ref{Fig:MorpgQDiverQP}). 
        \item Higher code coverage vs QDiff: 8.1\% vs 6.1\% 
    \end{itemize}
    \item 13 bugs discovered in the latest version of Qiskit.
    \item Follow-up QP behaviour analysis:
    \begin{itemize}
        \item Crashed in 23.2\% of the cases,with only 56 programs showing a distribution difference.
        \item Demonstrated wider diversity compared to QDiff, as measured by unique API calls.
        \item \textit{Roundtrip conversion via QASM} and \textit{Inject null-effect operations} are the most effective MR, althogh a combination of several MR will produce better results, exposing 8 out of 13 bugs (Figure \ref{Fig:MorpgQDiverQP}). 
        \item Non-crashing follow-up QP: Upon evaluating the 56 programs, it was determined that the differences in distribution were due to randomness, which is plausible given the test's significance level.
    \end{itemize}
    \item False positive: MorphQ may generate false positive warnings because the MR do not always hold in practice, even when is theoretically sound. This can occur when applying the \textit{Change of gate set}. In Qiskit, A* algorithm is used to find equivalent gates, although exploring all possibilities is impractical. Therefore, we might encounter the warning \textit{"Unable to map source basis to target basis"}, which  could be recognised as a limitation of the platform.
\end{itemize}

\begin{figure}[!tbp]
  \centering
  \begin{minipage}[b]{0.36\textwidth}
    \includegraphics[width=\textwidth]{TFM/photos/MorpgQDiverQP.png}
    \caption{QP diversity \cite{paltenghi2023morphq}.} 
    \label{Fig:MorpgQDiverQP}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.55\textwidth}
    \includegraphics[width=\textwidth]{TFM/photos/MorphQMRCrash.png}
    \caption{MR analysis in follow-up QP \cite{paltenghi2023morphq}.} 
    \label{Fig:MorphQMRCrash}
  \end{minipage}
\end{figure}

\subsection{Quantum computing programs}
\label{Ch2.3.2:TQP}
After considering some of the latest results regarding testing of quantum platforms and the diverse challenges they have encountered and overcame, we will shift our focus towards testing our QP implementations. This presents a different challenge where we have an algorithm or specification and an implementation that should represent that algorithm. We will explore the recent developments and analyse different techniques used for testing an implementation (QP) of a given algorithm.

\vspace{10pt}
\subsubsection{QSharpCheck}
\label{Ch2.3.2:QSharpCheck}
The initial approach we will discuss was presented by S. Honarvar et al. in 2020 with the title: "\textit{Property-based Testing of Quantum Programs in $Q\#$}" \cite{honarvar2020property}. The authors position this approach as the first step towards structured testing techniques for quantum programs, offering a property-based framework tailored for $Q\#$. Their work was influenced by Quantum Hoare Logic \cite{ying2012floyd} and assertion language \cite{huang2019statistical}. The authors introduce a syntax for specifying properties of $Q\#$ programs along with a tool named QSharpCheck, designed for test case generation, text execution, and analysis of outcomes.\newline

Let's delve into the syntax introduced for property specification. This involves associating a test property with a name and parameters, followed by allocation and setup, function call and finally assertion. This structured syntax is illustrated in Figure \ref{Fig:QSharpSyntax}, and an example is provided in Figure \ref{Fig:QSharpSyntaxEx}.

\vspace{15pt}

\begin{figure}[H]
        \centering
        \includegraphics[width=0.5\textwidth]{TFM/photos/QsharpSyntaxEx.png}
        \caption{Property example, state transformation \cite{honarvar2020property}.} 
        \label{Fig:QSharpSyntaxEx}
\end{figure}

\begin{figure}[H]
        \centering
        \includegraphics[width=0.5\textwidth]{TFM/photos/QsharpSyntax.png}
        \caption{Property syntax structure \cite{honarvar2020property}.} 
        \label{Fig:QSharpSyntax}
\end{figure}


With the established syntax, we can now introduce QSharpCheck, illustrated in Figure \ref{Fig:QSharpArch}. We could observe some output examples in Figure \ref{FIG:QSharpOutput}:

\begin{itemize}
    \item Test case generator. QSharpCheck functions as a test case generator, producing the specified number of test cases randomly within the range specified in the parameters.
    \item Test execution Engine. This component repeats the execution as many times as required by the specification.
    \item Statistical Analysis Engine. QSharpCheck incorporates a statistical analysis engine that performs hypothesis testing, considering the last assertion in the test as the null hypothesis. It assumes the program follows a binomial distribution, and in cases where the user doesn't provide a confidence level, it defaults to 0.99.
\end{itemize}

\begin{figure}[H]
        \centering
        \includegraphics[width=0.4\textwidth]{TFM/photos/QSharpOverview.png}
        \caption{QSharpCheck architecture \cite{honarvar2020property}.} 
        \label{Fig:QSharpArch}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}[H]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{TFM/photos/QSharpOut1.png}
        \caption{QSharpCheck positive outcome.} 
        \label{Fig:QSharpOutput1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[H]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{TFM/photos/Qsharpout2.png}
        \caption{QSharpCheck negative test.} 
        \label{Fig:QSharpOutput2}
    \end{subfigure}
        \caption{Two possible QSharpCheck outcomes \cite{honarvar2020property}. }
    \label{FIG:QSharpOutput}
 \end{figure}

The authors have designed QSharpCheck to accommodate various types of assertions as methods. Let's explore the types of assertions that can be used within this tool.

\begin{itemize}
    \item AssertProbability. This assertion observes if a given qubit can reach a specified measurement with a designated probability.
    \item AssertEntangled. Given two qubits as input, this assertion tests whether or not they are entangled.
    \item AssertEqual. As the name suggests, this assertion checks whether two qubits are equal. The null hypothesis is defined based on the equality of the qubits.
    \item AssertTeleported. Specifically designed for testing teleportation, this method takes the sent and received qubits as arguments. It analyses the probability found on the sent qubit and executes the \textit{AssertProbability} on the received qubit.
    \item AssertTransformed, This assertion is employed for unitary transformations.
\end{itemize}

To asses the capabilities of this framework, the authors introduce mutation testing to two programs, teleportation and superdense coding. The results yielded a mutation score of $80\%$ for teleportation and $60\%$ for superdense coding, with 20 mutant programs examined in each case.\newline

The authors conclude the article by summarising the completed work, emphasising that this marks the initial phase in property-based testing of QPs. The primary contribution of the article lies in the integration of a property language based on pre- and post-condition type properties and QSharpCheck.


\vspace{15pt}
\subsubsection{Quito}
\label{Ch2.3.2:Quito}
 The subsequent approach, presented by S. Ali et all in 2021 under the title: "\textit{Assessing the effectiveness of input and output coverage criteria for testing quantum programs}"\cite{ali2021assessing}, introduces the Quito approach (QUantum InpuT Output coverage). The authors define three coverage criteria on the inputs and outputs of QPs along with their test generation strategies.\newline

In their paper, the authors formally establish definitions for QP, input, output, program specification, valid output values, and test input, output, and suite. As well, the definition of input, output and input-output criteria follows the usual such that every input/output/<input,output>, respectively,  is covered by a test suite. Let's delve in their definition for outcome of a test suite:

\vspace{8pt}
\begin{itemize}
    \item Definitely fail: The outcome returns a non-valid output.
    \item Likely fail: The tested probability significantly deviates from the expected probability based on the Wilcoxon test with a $99\%$ confidence interval.
    \item Inconclusive: No failure has been detected.
\end{itemize}

To proceed, the authors propose the application of two different oracles in sequence:

\vspace{-3pt}
\begin{itemize}
    \item Wrong Output Oracle, WOO: Checks if QP only produces valid output values.
    \item Output Probability Oracle, OPO: Verifies if the QP returns an expected output with its associated probability. Providing one of the outcomes explained previously.
\end{itemize}

Let us introduce the test suit generators for testing.To initiate the process, the user will define a parameter $K$ representing the desired number of test suites. Additionally, a budget for output coverage need to be allocates as a constraint for generating a single test suite, because there is a chance of not finding the correct input for a particular output. The test suite creation algorithms will directly assess the WOO, halting if a non-valid output is produced and saving that test suite as a WOO failure.

\vspace{-3pt}
\begin{itemize}
    \item Input coverage. For each value up to $K$, each possible input will be executed and saved if it produces a valid output.
    \item Output coverage. For each value up to $K$, while there is elements in the valid outputs set and our budget for a single test suite generation has not been exhausted:
    \begin{itemize}
        \item[-] Execute one element from the valid input set.
        \item[-] Add $\langle$input, output$\rangle$ to the test suite.
        \item[-] Remove the output from the valid outputs set.
    \end{itemize}
    \item Input-Output coverage. For each value up to $K$ and for each valid input, we define the valid outputs set from the specification and proceed similarly to output coverage with this set and budget.
\end{itemize}

Therefore, we can now outline the Quito workflow:
\begin{itemize}
    \item Test suites generation.
    \item Check for WOO outcomes, already evaluated during test suite creation. The outcome will be \textit{definitely fail} if fails WOO.
    \item For test suites that pass WOO, they will be assessed by OPO. Since larger test suites may be required for statistical significance, it may be necessary to merge some test suites to meet this criterion. The outcome will be either \textit{likely fail} or \textit{inconclusive}.
\end{itemize}

The authors intend to assess these coverage criteria using mutation analysis, employing the following categories of mutation operators: add gate (AG), delete gate (DG), replace gate (RG), and replace mathematical operator (RMO), these operator behave similarly to classical mutation operators. The effectiveness of their test suites will be evaluated on mutated programs. Since results may vary across executions, each test suite will be executed K times for each criteria.\newline

A mutant will be considered "killed" if at least one of the proposed oracles fails. The mutation score is defined by subtracting the number of equivalent mutants from the total number of mutants. As the number of equivalent mutants is initially unknown, the authors start with 0 and analyse it after testing. The equivalent mutants are then studied manually, step by step, comparing states from the original process to a mutated one using the QCEngine execution facility.\newline

Let us summarise the key results and contributions of Quito:
\begin{itemize}
    \item Three coverage criteria: Quito introduces three coverage criteria that are independent of a specific language, each equipped with its own algorithm for creating test suites to achieve full coverage.
    \item Consistent Mutation Scores: The mutation scores tend to remain within the same range regardless of the coverage criteria. Although there are some variations, achieving higher mutation scores comes with a significant computational cost. For instance, RCR reaches an $80\%$ mutation score with 8000 test cases in input coverage and over 15000 in output coverage. However, it attains a $95\%$ mutation score on input-output coverage but requires 160k test cases.
    \item Equivalent Mutants: All non-killed mutants were identified as equivalent mutants, and some exhibited a phase difference.
\end{itemize}

To conclude this approach, let's consider some potential future work that is either underway or could be explored:

\begin{itemize}
    \item Testing QP phases: To obtain a reduction of test cases through boundary values and equivalence partitioning.
    \item There is no knowledge/approach about equivalent mutants in QP.
    \item There is no knowledge/guide regarding the appropriate setup of parameters for these coverage criteria.
\end{itemize}


\vspace{15pt}
\subsubsection{Muskit}
\label{Ch2.3.2:Muskit}
The third approach was introduced by E. Mendiluze et al. with the title: "\textit{Muskit: A Mutation Analysis Tool for Quantum Software Testing}"\cite{mendiluze2021muskit} in 2021. The authors present Muskit as the novel tool to autonomously implement mutation testing in Quantum Programs (QPs). While there were some previous works\cite{ali2021assessing}\cite{wang2021quito} utilising mutation testing in quantum, all mutated programs were manually generated.\newline

As evident from the Muskit architecture (Figure \ref{Fig:MuskitArch}), Muskit comprises two main components, with the added Test Analyser. Let's delve into these components. The first, and a key result of this paper, is the introduction of a mutation generator. Let's explore the various mutation operators employed and the criteria the tool uses to implement them.

\begin{figure}[H]
        \centering
        \includegraphics[width=0.46\textwidth]{TFM/photos/MuskitOverview.png}
        \caption{Muskit architecture \cite{mendiluze2021muskit}} 
        \label{Fig:MuskitArch}
\end{figure}

Mutation operators supported for 19 gates:
\begin{itemize}
    \item Add Gate (AG).
    \item Remove Gate (RemG).
    \item Replace Gate (RepG). Replacing an existing gate for one compatible, applied to the same number of qubits.
\end{itemize}

Mutation criteria, will reduce the number of mutants to be generated:
\begin{itemize}
    \item All. All possible mutants are generated.
    \item Operator Selections. The user has the flexibility to choose which mutant operators they would like to use.
    \item Gate Selection. The user controls mutant generation based on the number of qubits. Muskit supports three categories: one qubit gates, two qubit gates and three or more qubit gates. 
    \item Gate Number Selection. The user can control what gate will be mutated, as Muskit maps gates to numbers.
    \item Location Number. The user can locate where to apply AG.
    \item Phase Selection. The user can select a phase between 0 and 360 when mutants are applied to phase gates.
    \item Maximum Number. One can limit the number of mutants to be generated.
\end{itemize}

When focusing on the Mutants Executor component, it takes mutants and test cases as input, executing them as many times as specified in a file called \textit{ExecutorConfiguration}. In cases where no specification is provided for the input test cases, it defaults to using the \textit{Input coverage} as presented in \ref{Ch2.3.2:Quito}\cite{ali2021assessing} by S. Ali et al. The tool follows a similar approach for the Test Analyser component, importing both oracles—Wrong Output Oracle (WOO) and Output Probability Oracle (OPO)—from \ref{Ch2.3.2:Quito}\cite{ali2021assessing}. Once analysed, all results are reported.\newline

The authors conducted experiments using four different QPs to evaluate Muskit, yielding the results shown in Figure \ref{Fig:MuskitRes}, to demonstrate the capability of automatically generating mutants in QP

\begin{figure}[H]
        \centering
        \includegraphics[width=0.7\textwidth]{TFM/photos/MuskitResults.png}
        \caption{Muskit results \cite{mendiluze2021muskit}} 
        \label{Fig:MuskitRes}
\end{figure}

. Until now, all mutations had been performed manually. The paper identifies a limitation that Muskit currently faces, it is unable to detect equivalent mutants. The authors express concerns about the possibility that these quantum mutation operators may be related to each other in terms of the killability of their mutants. This leaves room for exploration in theoretical and experimental future works, presenting an area that could be further investigated.


\vspace{15pt}
\subsubsection{QMutPy}
\label{Ch2.3.2:QMutPy}
The initial approach we are presenting was authored by Fortunato, D. et all in 2022 in their article titled "\textit{Mutation testing of quantum programs: A case study with qiskit}"\cite{fortunato2022mutation}. In this work, they explore mutation testing on QP with the extension of the MutPy library by introducing new quantum mutation operators. This article can be divided into two distinct sections. The first section focuses on the introduction of QMutPy and the initial experiments. The second section delves deeper into the previous results to identify potential causes and solutions. The authors implement some of these solutions to determine if they are on the right path. \newline

First, let us understand what QP are going to be tested and why did the authors choose this library. The oracle problem was one of the challenges discussed earlier, where given an input, we need the expected behaviour. In this article, the authors have decided to prioritise and initially focus on this new approach for QP testing. To accomplish this, they opted to use Qiskit-Aqua's repository, which has since been moved to Qiskit-Terra \footnote{\url{https://github.com/Qiskit/qiskit-aqua/\#migration-guide}}.This repository provides the implementation of 24 QP along with their respective test suites. In these repository, you can discover a variety of programs, including pure classical, hybrid, and pure quantum programs.\newline

Similarly, they carefully selected the tool they believed would be the most suitable for implementing their ideas about quantum mutation testing. Their criteria included support for Python programs, testing frameworks, mutation operators, and the capability to generate appropriate reports. For that reason, they choose MutPy, as it fulfils their specific needs. Now, let us see how they develop these new quantum mutation operators, expanding the library mentioned earlier and naming it QMutPy.\newline

They have introduced in QMutPy 5 quantum mutation operators, QMO:
\begin{itemize}
    \item Quantum gate replacement, QRG.
    \item Quantum gate deletion, QRD.
    \item Quantum gate insertion, QRI.
    \item Quantum measurement insertion, QMI.
    \item Quantum measurement deletion, QMD.
\end{itemize}

\vspace{5pt}
These new operators are designed to capture the main errors that can occur during the implementation of an algorithm. The key concept around QMO is gate equivalence. We will consider two gates to be syntactically equivalent if and only if the number and type of the arguments are identical. The authors have identified 40 gates with at least one syntactical equivalent gate. As an example, gate h would have the following equivalent gates: x, y, z, i, id, s, sdg, sx, t and tdg. In their article \cite{fortunato2022mutation}, the authors illustrated all equivalence gates in Figure 1.  \newline

There is no need to provide an explanation for the mutant operators, as they are inherently self-explanatory.  However, let us dig into a more comprehensive understanding of gate operators. As mentioned earlier, the fundamental concept that makes these operators effective is gate equivalence. Whenever we encounter a gate addition (denoted by ".append()"), and we intend to employ QRG or QRI, we will either replace the existing gate with a syntactically equivalent one or simply add a new gate that preserves the syntax of our program. The process will generate as many mutants as there are syntactically equivalent gates. All of these modifications are implemented using Python's Abstract Syntax Tree (AST).\newline

QMutPy workflow, analogous to MutPy:
\begin{itemize}
    \item Loads QP source code and test suite.
    \item Executes test suite on the unmutated QP.
    \item Applies mutant operators, including QMO.
    \item Executes test suite on all mutated QP and provides a summary of results.
\end{itemize}

Because the works in quantum mutation \cite{wang2021qdiff}\cite{mendiluze2021muskit}\cite{ali2021assessing} were in a preliminary state, the authors opted to compare MutPy and QMutPy by running experiments on the same set of quantum programs and test suites. The experiment's metrics will be assessed through two distinct approximations of the mutation score. The first one follows the classical interpretation\cite{jia2010analysis}, while the second allows for the potential achievement of 100\% mutantion score. This will be achieved by excluding from the total mutants generated those that haven't been executed.\newline

The authors will perform when needed Kruskal-Wallist non-parametric test with $p=0.01$ and Cohen's $d$ effect-size measure on the results reported by QMutPy to evaluate the statistical significance.\newline

Let us conclude by highlighting the key contributions and outcomes of these experiments:

\begin{itemize}
    \item Expansion of the set of mutation operators for quantum programs, implemented in QMutPy.
    \item Introduction of gate equivalence definition based on the number and type of arguments.
    \item QMutPy generates quantum mutants for 11 out of 24 QPs. The remaining 13 QPs should exclude quantum gates and measurements. On average, 4 lines of code (LOCs) are mutated, resulting in 13 mutants and 64 mutants per QP. MutPy, on the other hand, generates mutants for all programs with an average of 64 LOCs mutated, 3 mutants, and 147 mutants per QP.
    \item QMutPy's performance in mutant creation is not statistically significantly lower than MutPy when considering the potential number of classical vs quantum mutations per QP.
    \item Test suits may only focus in the quantum part of the program, as evidenced by the difference in mutation scores between classical and quantum mutations and the type of killing (error vs test assertion).
    \item QMutPy exhibits reduced efficiency in creating quantum mutants. One proposed solution involves defining new operations in Python AST for quantum gates to enhance creation time.
\end{itemize}

As mentioned earlier, the authors delved into potential enhancements for these results, particularly focusing on test suites. They concentrated on two criteria for improvement: coverage and test assertions. By selecting specific programs for each test, they identified why mutants survived and implemented new assertions or tests as necessary. Therefore, the augmentation of test suites and assertions resulted in an increased mutation score for the studied programs.

\begin{itemize}
    \item The mutation score for $hhl$’s rose from $50\%$ to $100\%$ (with coverage increasing from $86.55\%$ to $89.16\%$), and for $vqc$’s, it increased from $0\%$ to $50\%$ (coverage rising from $93.26\%$ to $94.43\%$).
    \item In the case of Shor's algorithm, the original test suite achieved a $53.34\%$ mutation score, whereas the augmented test suite reached $72.81\%$.
\end{itemize}

\section{Future work}
\label{Ch2.4:FutureWork}
